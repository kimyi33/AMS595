{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31eccd23",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression with Numpy and Graphing with Matplotlib (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5a3912",
   "metadata": {},
   "source": [
    "In class, we implemented a logistic regression model to perform binary classification. Here, I provided the exact same code that we discussed in class, but the values for learning_rate and num_epochs are modified. I also removed all the comments. You task is to\n",
    "1. Tune the Learning Rate and Number of Epochs: Your first task is to fine-tune the values for the learning_rate and num_epochs parameters. Your goal is to identify suitable values that will enable us to converge to a set of parameter values $\\theta$ closely approximating the optimal parameter values $\\theta^*$.\n",
    "2. Code Explanation: To enhance code comprehension, please augment the code with meaningful comments. These comments should elucidate the purpose and functionality of each code segment, making it easier for readers to understand the logistic regression implementation.\n",
    "\n",
    "By accomplishing these tasks, we aim to achieve a better understanding of the logistic regression model's behavior and its parameter optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc800a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "np.random.seed(595)\n",
    "X = np.random.rand(2, 100)\n",
    "w_true = np.array([1.5, -2.5])\n",
    "b_true = 1.0  # True bias\n",
    "probabilities = sigmoid(np.dot(w_true.T, X) + b_true)\n",
    "Y = (probabilities > 0.5).astype(int)\n",
    "X = X + 0.3 * np.random.rand(2, 100) - 0.1 * np.random.rand(2, 100)\n",
    "\n",
    "X_train, X_test = X[:,:80], X[:,80:]\n",
    "Y_train, Y_test = Y[:80], Y[80:]\n",
    "\n",
    "w = np.zeros(X.shape[0])\n",
    "b = 0.0\n",
    "learning_rate = 5.95\n",
    "num_epochs = 595\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    A_train = sigmoid(np.dot(w.T, X_train) + b)\n",
    "\n",
    "    dJdw = np.dot(X_train, (A_train - Y_train).T) / len(Y_train)\n",
    "    dJdb = np.mean(A_train - Y_train)\n",
    "\n",
    "    w -= learning_rate * dJdw\n",
    "    b -= learning_rate * dJdb\n",
    "\n",
    "A_train = sigmoid(np.dot(w.T, X_train) + b)\n",
    "predictions_train = (A_train > 0.5).astype(int)\n",
    "\n",
    "A_test = sigmoid(np.dot(w.T, X_test) + b)\n",
    "predictions_test = (A_test > 0.5).astype(int)\n",
    "\n",
    "train_accuracy = np.mean(predictions_train == Y_train)\n",
    "\n",
    "test_accuracy = np.mean(predictions_test == Y_test)\n",
    "\n",
    "print(f\"Training Set Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7157131",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[0, :80], X[1,:80], c=Y[:80], cmap=plt.cm.Paired)\n",
    "plt.xlim(-0.2, 1.2)\n",
    "plt.ylim(-0.2, 1.2)\n",
    "ax = plt.gca()\n",
    "\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = np.dot(w.T, np.c_[xx.ravel(), yy.ravel()].T) + b\n",
    "Z = sigmoid(Z)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, colors='k', levels=[0.5], alpha=0.5, linestyles=['--'])\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Logistic Regression Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73992c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[0, 80:], X[1, 80:], c=Y[80:], cmap=plt.cm.Paired)\n",
    "plt.xlim(-0.2, 1.2)\n",
    "plt.ylim(-0.2, 1.2)\n",
    "\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50))\n",
    "Z = np.dot(w.T, np.c_[xx.ravel(), yy.ravel()].T) + b\n",
    "Z = sigmoid(Z)\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(xx, yy, Z, colors='k', levels=[0.5], alpha=0.5, linestyles=['--'])\n",
    "\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('Logistic Regression Decision Boundary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19905130",
   "metadata": {},
   "source": [
    "## 5. Image Binary Classification (30 pts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e01f79",
   "metadata": {},
   "source": [
    "For Question 5 of the Python project, please complete it using two distinct Jupyter Notebook scripts. This entails using one notebook for the original dataset and another for the modified dataset. Consequently, you will be submitting a total of three .ipynb files as part of your Python project, ensuring clear separation and organization of your work.\n",
    "\n",
    "Your tasks:\n",
    "1. Your first task is to re-implement the classification model that was covered in our class. Please start from scratch and write the code independently. You can refer to the original code provided on Brightspace if you encounter any difficulties, but try to write the code on your own to reinforce your understanding.\n",
    "\n",
    "2. After implementing the classification model, **report the indices of all the images in the test set for which the model's predictions are incorrect.** To maintain consistency with Pythonâ€™s convention, please begin your indexing with 0. Additionally, **display the images of 4 of these failed cases for visual examination (you can display more if you like)**. This analysis might help us identify instances where the model is struggling in some applications.\n",
    "\n",
    "3. Now you will modify the code to treat the dataset differently. The first 160 images plus the last digit of your Student ID will constitute the new training set, while the remaining images in the original training set will be your new test set. There are 209 images in the original training set. For example, if your ID ends with 0, the first 160 images will form your training set, and the remaining 49 images will be part of your test set. The test dataset is discarded and no longer used. Re-train the model using this modified dataset and **report the training and test accuracies**. Additionally, **provide the indices of all the images in the test set for which the model's predictions are incorrect. Display 4 of these misclassified images for further examination.**\n",
    "\n",
    "By completing these tasks, you'll gain valuable insights into the classification model's performance and its behavior under different training and testing conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7436a318",
   "metadata": {},
   "source": [
    "\n",
    "  Datasets: [train]: train_catvnoncat.h5, [test]: test_catvnoncat.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34075719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import torch.nn as nn\n",
    "import torch.optim \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e20843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, models\n",
    "\n",
    "# Pipeline for preprocessing of the image. \n",
    "transform = transforms.Compose([\n",
    "\n",
    "    # Convert a tensor or an ndarray to PIL Image\n",
    "    # Original array is H x W x C\n",
    "    # 3 channel, default mode is RGB\n",
    "    transforms.ToPILImage(),\n",
    "\n",
    "    # Resize into a fixed size same as ImageNet dataset\n",
    "    transforms.Resize((224, 224)),\n",
    "\n",
    "    # Flip the image horizontally using the default probability 0.5\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "\n",
    "    # Transform to pytorch tensor\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # Normalizes the pixel values using mean and std of the ImageNet dataset\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86c5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model. \n",
    "# ResNet from https://pytorch.org/vision/stable/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 1)  # Binary classification (1 output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Set hyperparameters '''\n",
    "\n",
    "# Use Binary Cross Entropy With Logits Loss for binary classification\n",
    "criterion = nn.BCEWithLogitsLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "# Set scheduler to adjust learning rate during training\n",
    "# StepLR reduce the learning rate by gamma after step_size of epoches\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=1e-6)\n",
    "\n",
    "model = model.to(device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
